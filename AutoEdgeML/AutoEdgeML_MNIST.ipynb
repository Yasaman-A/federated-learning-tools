{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AutoEdgeML-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "K1bPZwQ0ORwB"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasaman-A/federated-learning-tools/blob/main/AutoEdgeML/AutoEdgeML_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h6NtGeW9NwI"
      },
      "source": [
        "# Lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poeIo5NjWQns"
      },
      "source": [
        "!pip install -U tensorboard-plugin-profile\n",
        "!pip install --quiet --upgrade tensorflow_federated\n",
        "!pip install nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BKyHkMxKHfV"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import collections\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import tensorflow_federated as tff\n",
        "from datetime import datetime\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "%load_ext tensorboard\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psIg57Zmnyx_"
      },
      "source": [
        "log_base_dir = '/tmp/logs/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImcF6IYuRMa7"
      },
      "source": [
        "rm -rf \"/tmp/logs/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k35Emof6Lzo6"
      },
      "source": [
        "federated_accuracies = {}\n",
        "federated_order=[]\n",
        "federated_history = collections.defaultdict(list)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOmwWSEOF0sJ"
      },
      "source": [
        "# Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VLP98K6GKsJ"
      },
      "source": [
        "#@title String fields { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Enter your model and parameters. You will also need to provide two functions which will be described in the form below.\n",
        "\n",
        "#@markdown Central-approach specific parameters.\n",
        "EPOCHS =  25#@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Federated-Learning-approach specific parameters.\n",
        "NUM_FD_ROUNDS =  50#@param {type:\"integer\"}\n",
        "NUM_CLIENTS =  5#@param {type:\"integer\"}\n",
        "SPLIT_RANDOMLY = False #@param {typ: \"boolean\"}\n",
        "\n",
        "#@markdown Common parameters.\n",
        "\n",
        "BATCH_SIZE = 50 #@param {type:\"integer\"}\n",
        "SHUFFLE_BUFFER = 1024 #@param {type:\"integer\"}\n",
        "TRAIN_SIZE = 0.685 #@param {type:\"number\"}\n",
        "VALIDATION_SIZE = 0.2 #@param {type:\"number\"}\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "#@markdown Provide the URL (web or local) to the model and the name of the extracted folder.\n",
        "\n",
        "MODEL_URL = \"file:////content/demo_model.tar.gz\" #@param {type:\"string\"}\n",
        "MODEL_EXTRACTED_DIR_NAME = \"demo_model\"  #@param {type:\"string\"}\n",
        "\n",
        "train_size = TRAIN_SIZE\n",
        "validation_size = VALIDATION_SIZE\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj4LFJ9gFZK9"
      },
      "source": [
        "Provide the dataset to use in the training. You can download a dataset from https://www.tensorflow.org/datasets/catalog/overview or create your own datasets. In either case, you should assign your dataset to a varibale called `dataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcKlG4x6FuKz",
        "outputId": "fd6f07de-8425-4e08-f1b0-081b00d59dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# An example of creating your own dataset from mnist images and some auxilary data.\n",
        "from random import randrange\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x = np.concatenate([x_train, x_test], axis=0).astype('float32')\n",
        "x /= 255\n",
        "x = x.reshape(x.shape[0], 28, 28, 1)\n",
        "\n",
        "NUM_CLASS = 10\n",
        "y = np.concatenate([y_train, y_test], axis=0)\n",
        "y = keras.utils.to_categorical(y, NUM_CLASS)\n",
        "\n",
        "# aux is used for non-iid. splitting the data randomly between 600 users for testing.\n",
        "aux = np.array([randrange(600) for i in range(len(y))])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x, aux, y))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhogmj5PK5w-"
      },
      "source": [
        "The code will run the following function to choose non-iid input. The function does this by receiving a `client_id` parameter as the first input and the rest of your dataset as the subsequent inputs. In this function you have the freedom to implememnt how you want the code to distribute the data between different clients. For example, similar to before, assume that your dataset has image data, a label, and some auxilary data. In which case, you can write the function as follows.\n",
        "\n",
        "```Python\n",
        "def get_non_iid(data):\n",
        "  return data[1].numpy()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y69ZGaGfL0My"
      },
      "source": [
        "def get_non_iid(data):\n",
        "  return # TODO: return the values of the target column. \n",
        " "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkUEVKOWJ0AP"
      },
      "source": [
        "The code will run the following function to map the elements of your dataset to a format that is acceptable for your model. For example, assuming that each element in your dataset contains an image data, a label, and some auxilary data you will need to write the following function.\n",
        "\n",
        "```python\n",
        "def prep_model_input(image, aux, label):\n",
        "  return (image, label)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w01HI4evJyEe"
      },
      "source": [
        "def prep_model_input(): # TODO: add inputs\n",
        "  # TODO perform optional modifications\n",
        "  return # TODO return the data suitable for your model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HntUgIroTt6k"
      },
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHrmYZBFOfyi",
        "outputId": "9e99ef8c-a063-4c40-cef7-a11326c07fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Download the dataset and model.\n",
        "\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "ftpstream = urllib.request.urlopen(MODEL_URL)\n",
        "thetarfile = tarfile.open(fileobj=ftpstream, mode=\"r|gz\")\n",
        "thetarfile.extractall()\n",
        "\n",
        "central_model = tf.keras.models.load_model(MODEL_EXTRACTED_DIR_NAME)\n",
        "\n",
        "def create_keras_model():\n",
        "  federated_model = tf.keras.models.load_model(MODEL_EXTRACTED_DIR_NAME)\n",
        "  return federated_model\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG0xZa3sQ5z4"
      },
      "source": [
        "# Central"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zzZ519tid_0",
        "cellView": "both",
        "outputId": "c666fe46-15b3-4177-d47d-159f8e8a0e1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Split data into train, validation, and test.\n",
        "\n",
        "# -------- Internal tool code\n",
        "# -- Split into train, test, and validation\n",
        "\n",
        "def split_dataset(dataset: tf.data.Dataset, fraction: float):\n",
        "    data_percent = round(fraction * 100)\n",
        "    if not (0 <= data_percent <= 100):\n",
        "        raise ValueError(\"validation data fraction must be âˆˆ [0,1]\")\n",
        "\n",
        "    # count = 0\n",
        "    # for _ in dataset:\n",
        "    #   count += 1\n",
        "    # max_fraction_index = round(count * fraction)\n",
        "\n",
        "    dataset = dataset.enumerate()\n",
        "    remaining_dataset = dataset.filter(lambda f, data: f % 100 > data_percent)\n",
        "    fraction_dataset = dataset.filter(lambda f, data: f % 100 <= data_percent)\n",
        "\n",
        "    # remove enumeration\n",
        "    remaining_dataset = remaining_dataset.map(lambda f, data: data)\n",
        "    fraction_dataset = fraction_dataset.map(lambda f, data: data)\n",
        "\n",
        "    return fraction_dataset, remaining_dataset\n",
        "\n",
        "\n",
        "shuffled_dataset = dataset.shuffle(SHUFFLE_BUFFER).map(prep_model_input)\n",
        "\n",
        "train_dataset, test_dataset = split_dataset(shuffled_dataset, train_size)\n",
        "train_dataset, validation_dataset = split_dataset(train_dataset, 1-validation_size)\n",
        "\n",
        "train_dataset      = train_dataset.batch(BATCH_SIZE)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "test_dataset       = test_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "validation_dataset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None, 28, 28, 1), (None, 10)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz-rZcpslq4M",
        "cellView": "both"
      },
      "source": [
        "\n",
        "#@title Compile and run the model.\n",
        "\n",
        "logdir = log_base_dir + \"central/\"\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "# -------- Run the model\n",
        "central_model.compile(loss=keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=0.02),\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Accuracy()])\n",
        "\n",
        "import time\n",
        "first = time.time()\n",
        "\n",
        "history = central_model.fit(train_dataset,\n",
        "          epochs=EPOCHS,\n",
        "          shuffle=True,\n",
        "          verbose=1,\n",
        "          validation_data=validation_dataset,\n",
        "          callbacks=[tensorboard_callback])\n",
        "\n",
        "seconds = time.time()\n",
        "print(\"Time diff =\", seconds - first)\n",
        "\n",
        "score = central_model.evaluate(test_dataset, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJ07S-9W9xJ"
      },
      "source": [
        "central_history = history.history"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xM1pPYjitOu"
      },
      "source": [
        "# Federated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMGPHOq-Bpdy"
      },
      "source": [
        "non_iid_map = collections.defaultdict(list)\n",
        "\n",
        "for data in iter(dataset):\n",
        "    non_iid_map[get_non_iid(data)].append(data)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJr-OypvD-OI"
      },
      "source": [
        "client_ids = [i for i in range(NUM_CLIENTS)]\n",
        "\n",
        "def create_non_iid_dataset_for_client_fn(client_id):\n",
        "    train = collections.defaultdict(list)\n",
        "    test = collections.defaultdict(list)\n",
        "\n",
        "    max_train_index = round(len(non_iid_map[client_id]) * TRAIN_SIZE)\n",
        "    for i, data in enumerate(non_iid_map[client_id]):\n",
        "        for index, item in enumerate(data):\n",
        "            if i < max_train_index:\n",
        "                train[index].append(item)\n",
        "            else:\n",
        "                test[index].append(item)\n",
        "\n",
        "    new_train_format = (train[0], train[1], train[2])\n",
        "    new_test_format = (test[0], test[1], test[2])\n",
        "    return tf.data.Dataset.from_tensor_slices(new_train_format),  tf.data.Dataset.from_tensor_slices(new_test_format), \n",
        "\n",
        "def fd_preprocess(dataset):\n",
        "  train_dataset, test_dataset = dataset\n",
        "  def batch_format_fn(x, y):\n",
        "    return collections.OrderedDict(\n",
        "        x = x,\n",
        "        y = y\n",
        "    )\n",
        "\n",
        "  return train_dataset.repeat(EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(prep_model_input).map(batch_format_fn), \\\n",
        "         test_dataset.repeat(EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).map(prep_model_input).map(batch_format_fn)\n",
        "\n",
        "def make_federated_data(client_ids):\n",
        "    train_dataset = []\n",
        "    test_dataset = []\n",
        "    for x in client_ids:\n",
        "        train, test = fd_preprocess(create_non_iid_dataset_for_client_fn(x))\n",
        "        train_dataset.append(train)\n",
        "        test_dataset.append(test)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "train_preprocessed_fd_dataset, test_preprocessed_fd_dataset = make_federated_data(client_ids)\n",
        "element_spec = train_preprocessed_fd_dataset[0].element_spec\n",
        "train_preprocessed_fd_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh6geGJfjbyJ"
      },
      "source": [
        "count = 0\n",
        "for ds in iter(train_preprocessed_fd_dataset[0]):\n",
        "    count += 1\n",
        "    #print(count)\n",
        "print(count)\n",
        "\n",
        "count = 0\n",
        "for ds in iter(test_preprocessed_fd_dataset[0]):\n",
        "    count += 1\n",
        "    #print(count)\n",
        "print(count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laLnSynTtzia",
        "cellView": "both"
      },
      "source": [
        "#@title Prep the model and log dir.\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=element_spec,\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(name=\"epoch_loss\"),\n",
        "      metrics=[\n",
        "               tf.keras.metrics.CategoricalAccuracy(name=\"epoch_categorical_accuracy\"),\n",
        "               tf.keras.metrics.Accuracy(name=\"epoch_accuracy\")\n",
        "               ])\n",
        "  \n",
        "import shutil\n",
        "import pathlib\n",
        "\n",
        "fd_key = \"clients_{}_rounds_{}_splitrandom_{}\".format(NUM_CLIENTS, NUM_FD_ROUNDS, SPLIT_RANDOMLY)\n",
        "train_logdir = log_base_dir + \"federated/train_{}\".format(fd_key)\n",
        "test_logdir = log_base_dir + \"federated/test_{}\".format(fd_key)\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(train_logdir)\n",
        "  shutil.rmtree(test_logdir)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "pathlib.Path(train_logdir).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(test_logdir).mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDfMOcGLb_-y"
      },
      "source": [
        "#@title Compile and run the model.\n",
        "\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(train_logdir)\n",
        "\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "# TODO make this a parametr as well.\n",
        "USERS_PER_ROUND = NUM_CLIENTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J08HAdbUuMrq",
        "cellView": "both"
      },
      "source": [
        "import time\n",
        "\n",
        "start_time = time.monotonic()\n",
        "print(time.ctime())\n",
        "\n",
        "key = \"C{},R{}\".format(NUM_CLIENTS, NUM_FD_ROUNDS)\n",
        "\n",
        "with summary_writer.as_default():\n",
        "    for round_num in range(0, NUM_FD_ROUNDS+1):\n",
        "        round_start_time = time.monotonic()\n",
        "        state, metrics = iterative_process.next(state, train_preprocessed_fd_dataset)\n",
        "        #print('Round duration: ', (time.monotonic() - round_start_time))\n",
        "        federated_history[key].append(metrics['train'])\n",
        "        for name, value in metrics['train'].items():\n",
        "            if name == 'loss':\n",
        "                name = 'epoch_loss'\n",
        "            tf.summary.scalar(name, value, step=round_num)\n",
        "        print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "print('Duration: ', (time.monotonic() - start_time))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b95bW2k6dxzw"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
        "\n",
        "train_metrics = evaluation(state.model, train_preprocessed_fd_dataset)\n",
        "test_metrics = evaluation(state.model, test_preprocessed_fd_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL-zp_U1eNO6"
      },
      "source": [
        "print(train_metrics)\n",
        "print(test_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw-PgZscMD8r"
      },
      "source": [
        "key = \"C{},R{}\".format(NUM_CLIENTS, NUM_FD_ROUNDS)\n",
        "federated_accuracies[key] = [test_metrics['epoch_categorical_accuracy']]\n",
        "federated_order.append(key)\n",
        "federated_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI6kP1hw4Am9"
      },
      "source": [
        "federated_order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIgV9BIGO5Vt"
      },
      "source": [
        "# Show Graphs with TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B34eDDIb0vuu",
        "cellView": "both"
      },
      "source": [
        "%tensorboard --logdir '/tmp/logs/' --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9uYuoWaq6UR"
      },
      "source": [
        "# Draw Graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A61SAbfJjFbG"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import random"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqkkKnyFgZKI"
      },
      "source": [
        "for key in federated_accuracies:\n",
        "    for i in range(len(federated_accuracies[key])):\n",
        "        federated_accuracies[key][i] *= 100"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t1mkr2e_k0d"
      },
      "source": [
        "\n",
        "\n",
        "values = {}\n",
        "for key in federated_order:\n",
        "    values[key] = federated_accuracies[key]\n",
        "\n",
        "#df2 = pd.DataFrame.from_dict(federated_accuracies)\n",
        "df2 = pd.DataFrame.from_dict(values)\n",
        "\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "ax = sns.barplot(data=df2, palette=\"muted\", edgecolor=(0,0,0))\n",
        "ax.set(ylim = (0,110))\n",
        "\n",
        "ax.set(ylabel='Accuracy (%)')\n",
        "\n",
        "i = 0\n",
        "for key in federated_accuracies:\n",
        "    ax.text(i,\n",
        "            federated_accuracies[key][0] + 1,\n",
        "            \"{:.2f}%\".format(federated_accuracies[key][0]),\n",
        "            color='black',\n",
        "            ha=\"center\")\n",
        "    i += 1\n",
        "\n",
        "def change_width(ax, new_value) :\n",
        "    for patch in ax.patches :\n",
        "        current_width = patch.get_width()\n",
        "        diff = current_width - new_value\n",
        "\n",
        "        # we change the bar width\n",
        "        patch.set_width(new_value)\n",
        "\n",
        "        # we recenter the bar\n",
        "        patch.set_x(patch.get_x() + diff * .5)\n",
        "\n",
        "change_width(ax, .5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6eW99yOR50-"
      },
      "source": [
        "\n",
        "\n",
        "central_acc = score[1] * 100\n",
        "fd_acc = test_metrics['epoch_categorical_accuracy'] * 100\n",
        "df2 = pd.DataFrame(np.array([[central_acc, fd_acc]]), columns=['Central', 'Federated'])\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "\n",
        "ax = sns.barplot(data=df2, palette=\"muted\", edgecolor=(0,0,0))\n",
        "ax.set(ylim = (0,110))\n",
        "\n",
        "ax.set(ylabel='Accuracy (%)')\n",
        "\n",
        "ax.text(0, central_acc + 1, \"{:.2f}%\".format(central_acc), color='black', ha=\"center\")\n",
        "ax.text(1, fd_acc + 1, \"{:.2f}%\".format(fd_acc), color='black', ha=\"center\")\n",
        "\n",
        "change_width(ax, .5)\n",
        "\n",
        "plt.show()\n",
        "#sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO-EuZpfSPq1"
      },
      "source": [
        "federated_categorical = collections.defaultdict(list)\n",
        "for key in federated_history:\n",
        "    for i in range(len(federated_history[key])):\n",
        "        federated_categorical[key].append(federated_history[key][i]['epoch_categorical_accuracy'] * 100)\n",
        "\n",
        "fd_len = len(federated_categorical[list(federated_categorical.keys())[0]])\n",
        "for key in federated_categorical:\n",
        "    fd_len = min(len(federated_categorical[key]), fd_len)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBJTO3Q4Zb-4"
      },
      "source": [
        "central_categorical = central_history['categorical_accuracy'] * 100\n",
        "central_len = len(central_history['categorical_accuracy'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jACVCRB2d8PF"
      },
      "source": [
        "count = fd_len#min(fd_len, central_len)\n",
        "ids = [i for i in range(count)]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbsE2NqgcHbf"
      },
      "source": [
        "d = {'id': ids}\n",
        "for key in federated_categorical:\n",
        "    d[key] = federated_categorical[key][:count]\n",
        "df = pd.DataFrame(data=d)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR6b1h_SoCKm"
      },
      "source": [
        "\n",
        "\n",
        "max_line_count = 10\n",
        "\n",
        "colors = random.sample(list(mcolors.TABLEAU_COLORS.keys()), max_line_count)\n",
        "markers = random.sample([ \".\", \",\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\"], max_line_count)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4BOCl-xc7Wg"
      },
      "source": [
        "\n",
        "#plt.plot( 'id', 'central', data=df, markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\n",
        "i = 0\n",
        "for key in federated_categorical:\n",
        "    plt.plot( 'id', key, data=df, marker=markers[i], markerfacecolor=colors[i], color=colors[i], linewidth=1)\n",
        "    i += 1\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.grid(axis='x')\n",
        "\n",
        "plt.ylim(0, 100)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}